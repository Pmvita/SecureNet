#!/usr/bin/env python3
"""
Week 3 Day 3 Validation: Customer Onboarding Automation
Comprehensive validation of customer onboarding system
"""

import asyncio
import json
import logging
import sys
from datetime import datetime, timezone
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class Week3Day3Validator:
    """Week 3 Day 3 Customer Onboarding Automation Validator"""
    
    def __init__(self):
        self.total_points = 100
        self.results = {
            "validation_timestamp": datetime.now(timezone.utc).isoformat(),
            "week": 3,
            "day": 3,
            "focus": "Customer Onboarding Automation",
            "total_possible_points": self.total_points,
            "components": {
                "onboarding_orchestrator": {"max_points": 25, "earned_points": 0, "tests": []},
                "customer_journey_automation": {"max_points": 25, "earned_points": 0, "tests": []},
                "analytics_and_tracking": {"max_points": 25, "earned_points": 0, "tests": []},
                "self_service_capabilities": {"max_points": 25, "earned_points": 0, "tests": []}
            }
        }
    
    async def run_validation(self) -> dict:
        """Run complete Week 3 Day 3 validation"""
        logger.info("üîç Starting Week 3 Day 3 Customer Onboarding Automation Validation...")
        
        try:
            # Import the customer onboarding system
            from utils.week3_day3_customer_onboarding import Week3Day3CustomerOnboarding, CustomerProfile
            self.onboarding_system = Week3Day3CustomerOnboarding()
            
            # 1. Validate Onboarding Orchestrator (25 points)
            orchestrator_score = await self._validate_onboarding_orchestrator()
            self.results["components"]["onboarding_orchestrator"]["earned_points"] = orchestrator_score
            
            # 2. Validate Customer Journey Automation (25 points)
            journey_score = await self._validate_customer_journey_automation()
            self.results["components"]["customer_journey_automation"]["earned_points"] = journey_score
            
            # 3. Validate Analytics and Tracking (25 points)
            analytics_score = await self._validate_analytics_and_tracking()
            self.results["components"]["analytics_and_tracking"]["earned_points"] = analytics_score
            
            # 4. Validate Self-Service Capabilities (25 points)
            self_service_score = await self._validate_self_service_capabilities()
            self.results["components"]["self_service_capabilities"]["earned_points"] = self_service_score
            
            # Calculate final results
            self._calculate_final_results()
            
            return self.results
            
        except Exception as e:
            logger.error(f"Validation failed: {e}")
            self.results["error"] = str(e)
            return self.results
    
    async def _validate_onboarding_orchestrator(self) -> int:
        """Validate Onboarding Orchestrator (25 points)"""
        logger.info("üéØ Validating Onboarding Orchestrator...")
        score = 0
        tests = []
        
        try:
            # Test 1: Orchestrator Initialization (5 points)
            if hasattr(self.onboarding_system, 'orchestrator'):
                score += 5
                tests.append({"test": "Orchestrator Initialization", "status": "PASS", "points": 5})
                logger.info("  ‚úÖ Onboarding orchestrator initialized")
            else:
                tests.append({"test": "Orchestrator Initialization", "status": "FAIL", "points": 0})
                logger.warning("  ‚ùå Orchestrator initialization failed")
            
            # Test 2: Template Configuration (5 points)
            templates = self.onboarding_system.orchestrator.templates
            if len(templates) >= 3 and all(key in templates for key in ["enterprise", "mid_market", "small_business"]):
                score += 5
                tests.append({"test": "Template Configuration", "status": "PASS", "points": 5})
                logger.info(f"  ‚úÖ {len(templates)} onboarding templates configured")
            else:
                tests.append({"test": "Template Configuration", "status": "FAIL", "points": 0})
                logger.warning("  ‚ùå Template configuration incomplete")
            
            # Test 3: Customer Onboarding Creation (5 points)
            from utils.week3_day3_customer_onboarding import CustomerProfile
            test_customer = CustomerProfile(
                customer_id="test_001",
                email="test@example.com",
                company_name="Test Company",
                industry="technology",
                company_size="51-200 employees",
                use_case="security_monitoring",
                technical_expertise="intermediate"
            )
            
            session_id = await self.onboarding_system.orchestrator.start_customer_onboarding(test_customer)
            if session_id and session_id in self.onboarding_system.orchestrator.onboarding_sessions:
                score += 5
                tests.append({"test": "Customer Onboarding Creation", "status": "PASS", "points": 5})
                logger.info("  ‚úÖ Customer onboarding session created successfully")
            else:
                tests.append({"test": "Customer Onboarding Creation", "status": "FAIL", "points": 0})
                logger.warning("  ‚ùå Customer onboarding creation failed")
            
            # Test 4: Automation Rules (5 points)
            automation_rules = self.onboarding_system.orchestrator.automation_rules
            required_rules = ["welcome_sequence", "progress_tracking", "stale_onboarding", "completion_celebration"]
            if all(rule in automation_rules for rule in required_rules):
                score += 5
                tests.append({"test": "Automation Rules", "status": "PASS", "points": 5})
                logger.info(f"  ‚úÖ {len(automation_rules)} automation rules configured")
            else:
                tests.append({"test": "Automation Rules", "status": "FAIL", "points": 0})
                logger.warning("  ‚ùå Automation rules incomplete")
            
            # Test 5: Task Management (5 points)
            if session_id:
                session = self.onboarding_system.orchestrator.onboarding_sessions[session_id]
                if len(session["tasks"]) > 0:
                    # Try to complete a task
                    first_task = session["tasks"][0]
                    result = await self.onboarding_system.orchestrator.complete_onboarding_task(
                        session_id, first_task.task_id, True
                    )
                    if result and "status" in result:
                        score += 5
                        tests.append({"test": "Task Management", "status": "PASS", "points": 5})
                        logger.info("  ‚úÖ Task completion working correctly")
                    else:
                        tests.append({"test": "Task Management", "status": "FAIL", "points": 0})
                        logger.warning("  ‚ùå Task completion failed")
                else:
                    tests.append({"test": "Task Management", "status": "FAIL", "points": 0})
                    logger.warning("  ‚ùå No tasks found in session")
            else:
                tests.append({"test": "Task Management", "status": "FAIL", "points": 0})
                logger.warning("  ‚ùå No session available for task testing")
        
        except Exception as e:
            tests.append({"test": "Onboarding Orchestrator Overall", "status": "FAIL", "points": 0, "error": str(e)})
            logger.error(f"  ‚ùå Orchestrator validation error: {e}")
        
        self.results["components"]["onboarding_orchestrator"]["tests"] = tests
        logger.info(f"üéØ Onboarding Orchestrator Score: {score}/25")
        return score
    
    async def _validate_customer_journey_automation(self) -> int:
        """Validate Customer Journey Automation (25 points)"""
        logger.info("üöÄ Validating Customer Journey Automation...")
        score = 0
        tests = []
        
        try:
            # Test 1: Journey Scenario Creation (5 points)
            scenarios = await self.onboarding_system.create_sample_onboarding_scenarios()
            if scenarios and "scenarios_created" in scenarios and scenarios["scenarios_created"] >= 2:
                score += 5
                tests.append({"test": "Journey Scenario Creation", "status": "PASS", "points": 5})
                logger.info(f"  ‚úÖ {scenarios['scenarios_created']} onboarding scenarios created")
            else:
                tests.append({"test": "Journey Scenario Creation", "status": "FAIL", "points": 0})
                logger.warning("  ‚ùå Journey scenario creation failed")
            
            # Test 2: Multi-Template Support (5 points)
            if scenarios and "templates_available" in scenarios:
                templates = scenarios["templates_available"]
                if len(templates) >= 3:
                    score += 5
                    tests.append({"test": "Multi-Template Support", "status": "PASS", "points": 5})
                    logger.info(f"  ‚úÖ {len(templates)} customer templates available")
                else:
                    tests.append({"test": "Multi-Template Support", "status": "FAIL", "points": 0})
                    logger.warning("  ‚ùå Insufficient templates available")
            else:
                tests.append({"test": "Multi-Template Support", "status": "FAIL", "points": 0})
                logger.warning("  ‚ùå Template information not available")
            
            # Test 3: Journey Simulation (5 points)
            if scenarios and "scenarios" in scenarios and len(scenarios["scenarios"]) > 0:
                first_session_id = scenarios["scenarios"][0]["session_id"]
                journey_result = await self.onboarding_system.simulate_onboarding_journey(first_session_id)
                
                if journey_result and "journey_completed_steps" in journey_result and journey_result["journey_completed_steps"] > 0:
                    score += 5
                    tests.append({"test": "Journey Simulation", "status": "PASS", "points": 5})
                    logger.info(f"  ‚úÖ Journey simulation completed {journey_result['journey_completed_steps']} steps")
                else:
                    tests.append({"test": "Journey Simulation", "status": "FAIL", "points": 0})
                    logger.warning("  ‚ùå Journey simulation failed")
            else:
                tests.append({"test": "Journey Simulation", "status": "FAIL", "points": 0})
                logger.warning("  ‚ùå No scenarios available for simulation")
            
            # Test 4: Progress Tracking (5 points)
            if scenarios and "scenarios" in scenarios:
                session_status = None
                for scenario in scenarios["scenarios"]:
                    if "status" in scenario:
                        session_status = scenario["status"]
                        break
                
                if session_status and "overall_progress" in session_status:
                    score += 5
                    tests.append({"test": "Progress Tracking", "status": "PASS", "points": 5})
                    logger.info(f"  ‚úÖ Progress tracking operational ({session_status['overall_progress']:.1f}%)")
                else:
                    tests.append({"test": "Progress Tracking", "status": "FAIL", "points": 0})
                    logger.warning("  ‚ùå Progress tracking not working")
            else:
                tests.append({"test": "Progress Tracking", "status": "FAIL", "points": 0})
                logger.warning("  ‚ùå No scenarios available for progress testing")
            
            # Test 5: Automation Triggers (5 points)
            automation_rules = scenarios.get("automation_rules", []) if scenarios else []
            if len(automation_rules) >= 4:
                score += 5
                tests.append({"test": "Automation Triggers", "status": "PASS", "points": 5})
                logger.info(f"  ‚úÖ {len(automation_rules)} automation rules operational")
            else:
                tests.append({"test": "Automation Triggers", "status": "FAIL", "points": 0})
                logger.warning("  ‚ùå Insufficient automation rules")
        
        except Exception as e:
            tests.append({"test": "Customer Journey Automation Overall", "status": "FAIL", "points": 0, "error": str(e)})
            logger.error(f"  ‚ùå Journey automation validation error: {e}")
        
        self.results["components"]["customer_journey_automation"]["tests"] = tests
        logger.info(f"üöÄ Customer Journey Automation Score: {score}/25")
        return score
    
    async def _validate_analytics_and_tracking(self) -> int:
        """Validate Analytics and Tracking (25 points)"""
        logger.info("üìä Validating Analytics and Tracking...")
        score = 0
        tests = []
        
        try:
            # Test 1: Analytics System Initialization (5 points)
            if hasattr(self.onboarding_system, 'analytics'):
                score += 5
                tests.append({"test": "Analytics System Initialization", "status": "PASS", "points": 5})
                logger.info("  ‚úÖ Analytics system initialized")
            else:
                tests.append({"test": "Analytics System Initialization", "status": "FAIL", "points": 0})
                logger.warning("  ‚ùå Analytics system not found")
            
            # Test 2: Event Tracking (5 points)
            analytics = self.onboarding_system.analytics
            initial_customers = analytics.metrics["total_customers"]
            
            # Simulate tracking events
            await analytics.track_onboarding_event("onboarding_started", "test_session_123", {})
            await analytics.track_onboarding_event("task_completed", "test_session_123", {"task_id": "test_task"})
            
            if analytics.metrics["total_customers"] > initial_customers:
                score += 5
                tests.append({"test": "Event Tracking", "status": "PASS", "points": 5})
                logger.info("  ‚úÖ Event tracking working correctly")
            else:
                tests.append({"test": "Event Tracking", "status": "FAIL", "points": 0})
                logger.warning("  ‚ùå Event tracking not working")
            
            # Test 3: Success Score Calculation (5 points)
            # Create a mock session for testing
            mock_session = {
                "progress_percentage": 50.0,
                "started_at": datetime.now(timezone.utc),
                "tasks": [
                    type('Task', (), {
                        'status': type('Status', (), {'value': 'completed'})(),
                        'completion_rate': 1.0
                    })()
                ],
                "automation_triggers": [{"rule": "test", "triggered_at": datetime.now(timezone.utc)}]
            }
            
            success_score = await analytics.calculate_customer_success_score("test_session", mock_session)
            if success_score > 0:
                score += 5
                tests.append({"test": "Success Score Calculation", "status": "PASS", "points": 5})
                logger.info(f"  ‚úÖ Success score calculation working ({success_score:.1f})")
            else:
                tests.append({"test": "Success Score Calculation", "status": "FAIL", "points": 0})
                logger.warning("  ‚ùå Success score calculation failed")
            
            # Test 4: Analytics Insights (5 points)
            insights = await analytics.get_onboarding_insights()
            if insights and "overview" in insights and "optimization_opportunities" in insights:
                score += 5
                tests.append({"test": "Analytics Insights", "status": "PASS", "points": 5})
                logger.info("  ‚úÖ Analytics insights generation working")
            else:
                tests.append({"test": "Analytics Insights", "status": "FAIL", "points": 0})
                logger.warning("  ‚ùå Analytics insights generation failed")
            
            # Test 5: Metrics Collection (5 points)
            metrics = analytics.metrics
            required_metrics = ["total_customers", "active_onboardings", "completed_onboardings", "completion_rate_by_stage"]
            if all(metric in metrics for metric in required_metrics):
                score += 5
                tests.append({"test": "Metrics Collection", "status": "PASS", "points": 5})
                logger.info("  ‚úÖ Comprehensive metrics collection operational")
            else:
                tests.append({"test": "Metrics Collection", "status": "FAIL", "points": 0})
                logger.warning("  ‚ùå Metrics collection incomplete")
        
        except Exception as e:
            tests.append({"test": "Analytics and Tracking Overall", "status": "FAIL", "points": 0, "error": str(e)})
            logger.error(f"  ‚ùå Analytics validation error: {e}")
        
        self.results["components"]["analytics_and_tracking"]["tests"] = tests
        logger.info(f"üìä Analytics and Tracking Score: {score}/25")
        return score
    
    async def _validate_self_service_capabilities(self) -> int:
        """Validate Self-Service Capabilities (25 points)"""
        logger.info("üõ†Ô∏è Validating Self-Service Capabilities...")
        score = 0
        tests = []
        
        try:
            # Test 1: Self-Service System Initialization (5 points)
            if hasattr(self.onboarding_system, 'self_service'):
                score += 5
                tests.append({"test": "Self-Service System Initialization", "status": "PASS", "points": 5})
                logger.info("  ‚úÖ Self-service system initialized")
            else:
                tests.append({"test": "Self-Service System Initialization", "status": "FAIL", "points": 0})
                logger.warning("  ‚ùå Self-service system not found")
            
            # Test 2: Help Content Library (5 points)
            self_service = self.onboarding_system.self_service
            help_articles = self_service.help_articles
            if len(help_articles) >= 3:
                score += 5
                tests.append({"test": "Help Content Library", "status": "PASS", "points": 5})
                logger.info(f"  ‚úÖ {len(help_articles)} help articles available")
            else:
                tests.append({"test": "Help Content Library", "status": "FAIL", "points": 0})
                logger.warning("  ‚ùå Insufficient help content")
            
            # Test 3: Interactive Guides (5 points)
            interactive_guides = self_service.interactive_guides
            if len(interactive_guides) >= 2:
                score += 5
                tests.append({"test": "Interactive Guides", "status": "PASS", "points": 5})
                logger.info(f"  ‚úÖ {len(interactive_guides)} interactive guides available")
            else:
                tests.append({"test": "Interactive Guides", "status": "FAIL", "points": 0})
                logger.warning("  ‚ùå Insufficient interactive guides")
            
            # Test 4: Chatbot Responses (5 points)
            chatbot_responses = self_service.chatbot_responses
            if len(chatbot_responses) >= 4:
                score += 5
                tests.append({"test": "Chatbot Responses", "status": "PASS", "points": 5})
                logger.info(f"  ‚úÖ {len(chatbot_responses)} chatbot responses configured")
            else:
                tests.append({"test": "Chatbot Responses", "status": "FAIL", "points": 0})
                logger.warning("  ‚ùå Insufficient chatbot responses")
            
            # Test 5: Contextual Help (5 points)
            contextual_help = await self_service.get_contextual_help(
                "account_setup", 
                {"technical_expertise": "beginner"}
            )
            if contextual_help and "help_content" in contextual_help:
                score += 5
                tests.append({"test": "Contextual Help", "status": "PASS", "points": 5})
                logger.info("  ‚úÖ Contextual help system working")
            else:
                tests.append({"test": "Contextual Help", "status": "FAIL", "points": 0})
                logger.warning("  ‚ùå Contextual help system failed")
        
        except Exception as e:
            tests.append({"test": "Self-Service Capabilities Overall", "status": "FAIL", "points": 0, "error": str(e)})
            logger.error(f"  ‚ùå Self-service validation error: {e}")
        
        self.results["components"]["self_service_capabilities"]["tests"] = tests
        logger.info(f"üõ†Ô∏è Self-Service Capabilities Score: {score}/25")
        return score
    
    def _calculate_final_results(self):
        """Calculate final validation results"""
        total_earned = sum(component["earned_points"] for component in self.results["components"].values())
        success_rate = (total_earned / self.total_points) * 100
        
        self.results["total_earned_points"] = total_earned
        self.results["success_rate"] = round(success_rate, 1)
        self.results["validation_status"] = "PASS" if success_rate >= 80 else "NEEDS_IMPROVEMENT"
        
        # Component status
        for component_name, component_data in self.results["components"].items():
            component_success_rate = (component_data["earned_points"] / component_data["max_points"]) * 100
            component_data["success_rate"] = round(component_success_rate, 1)
            component_data["status"] = "PASS" if component_success_rate >= 80 else "NEEDS_IMPROVEMENT"
        
        logger.info(f"üìä Week 3 Day 3 Validation Complete: {total_earned}/{self.total_points} points ({success_rate:.1f}%)")

async def main():
    """Main validation execution"""
    validator = Week3Day3Validator()
    
    print("üîç Week 3 Day 3: Customer Onboarding Automation Validation")
    print("=" * 70)
    
    # Run validation
    results = await validator.run_validation()
    
    # Print results
    print(f"\nüìä VALIDATION RESULTS")
    print(f"Total Score: {results['total_earned_points']}/{results['total_possible_points']} ({results['success_rate']}%)")
    print(f"Status: {results['validation_status']}")
    
    print(f"\nüéØ COMPONENT BREAKDOWN:")
    for component_name, component_data in results["components"].items():
        status_emoji = "‚úÖ" if component_data["status"] == "PASS" else "‚ö†Ô∏è"
        print(f"{status_emoji} {component_name.replace('_', ' ').title()}: {component_data['earned_points']}/{component_data['max_points']} ({component_data['success_rate']}%)")
    
    # Save results
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = f"week3_day3_validation_{timestamp}.json"
    
    try:
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2, default=str)
        print(f"\nüíæ Results saved to: {results_file}")
    except Exception as e:
        print(f"\n‚ö†Ô∏è Could not save results: {e}")
    
    return results

if __name__ == "__main__":
    asyncio.run(main())